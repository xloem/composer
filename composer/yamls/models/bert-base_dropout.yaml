train_dataset:
  streaming_lm:
    dataset_name: c4
    dataset_config_name: en
    split: train
    max_shards: -1
    max_samples: 275184000 
    max_seq_len: 128
    group_method: truncate # TODO: check that the code for this one is right
    tokenizer_name: bert-base-uncased
    use_masked_lm: true # TODO: check that the code for this one is right 
    mlm_probability: 0.15
    seed: 17
    shuffle: true
    drop_last: true
val_dataset:
  streaming_lm:
    dataset_name: c4
    dataset_config_name: en
    split: validation
    max_shards: -1
    max_samples: 32000 # there's a bug, this is currently per-device 
    max_seq_len: 128
    group_method: truncate # TODO: check that the code for this one is right
    tokenizer_name: bert-base-uncased 
    use_masked_lm: true # TODO: check that the code for this one is right
    mlm_probability: 0.15
    seed: 17
    shuffle: false
    drop_last: true
model:
  bert:
    #pretrained_model_name: bert-base-uncased
    model_config:
      architectures:
        - BertForMaskedLM
      attention_probs_dropout_prob: 0.0
      gradient_checkpointing: false
      classifier_dropout: null
      hidden_act: gelu
      hidden_dropout_prob: 0.0
      hidden_size: 768
      initializer_range: 0.02
      intermediate_size: 3072
      layer_norm_eps: 1.0e-12
      max_position_embeddings: 512
      model_type: bert
      num_attention_heads: 12
      num_hidden_layers: 12
      pad_token_id: 0
      position_embedding_type: absolute
      type_vocab_size: 2
      use_cache: true
      vocab_size: 30522
    use_pretrained: false
    tokenizer_name: bert-base-uncased
optimizer:
  decoupled_adamw:
    lr: 5.0e-4
    betas:
      - 0.9
      - 0.98
    eps: 1.0e-06
    weight_decay: 1.0e-5
schedulers:
  linear_decay_with_warmup:
    warmup_time: 0.06dur
loggers:
  file:
    log_level: batch 
    filename: stdout
    buffer_size: 1
    flush_interval: 100
    log_interval: 100
max_duration: 1ep  # Baseline is 256M samples, 7 epochs is ~280M samples
log_level: INFO 
train_batch_size: 4000
eval_batch_size: 2000
seed: 19
device:
  gpu: {}
dataloader:
  pin_memory: true
  persistent_workers: true
  num_workers: 1
  timeout: 0
  prefetch_factor: 2
grad_accum: 1
precision: amp
grad_clip_norm: None
validate_every_n_batches: 1000
validate_every_n_epochs: 1
save_folder: bert_checkpoints/bert_dropout_00
save_interval: "100ba"
callbacks:
  run_directory_uploader:
    provider: "GOOGLE_STORAGE"
    container: "bert_checkpoints"
    key_environ: KEY
    secret_environ: SECRET
